{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess, ClippedCorpus\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models import LdaModel\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of song lyrics in dataset: 57650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57650"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to download song lyric dataset from https://www.kaggle.com/mousehead/songlyrics\n",
    "# and unzip it to ./data\n",
    "songs = pd.read_csv('./data/songdata.csv')\n",
    "songs_lyrics = songs.text.apply(lambda x: x.replace('\\n', ''))\n",
    "print('Number of song lyrics in dataset: {}'.format(len(songs_lyrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "    \n",
    "# create iterator over the documents (song lyrics)\n",
    "def corpus_iter():\n",
    "    for song in songs_lyrics:\n",
    "        yield tokenize(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 s, sys: 157 ms, total: 25.1 s\n",
      "Wall time: 25.4 s\n",
      "Dictionary(85449 unique tokens: ['believe', 'blue', 'face', 'feel', 'fellow']...)\n"
     ]
    }
   ],
   "source": [
    "# create dictionary from corpus\n",
    "%time song_dictionary = Dictionary(corpus_iter())\n",
    "print(song_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word IDs present in dictionary:\n",
      "[(808, 1), (2847, 1), (3412, 1), (5648, 1), (8740, 1), (12675, 1), (21085, 1)]\n",
      "['line', 'nature', 'rabbit', 'cell', 'mixed', 'reveals', 'cornea']\n"
     ]
    }
   ],
   "source": [
    "# sample sentence\n",
    "doc = \"Phenotypic characterization of the SIRC (Statens Seruminstitut Rabbit Cornea) cell line reveals a mixed epithelial and fibroblastic nature\"\n",
    "bow = song_dictionary.doc2bow(tokenize(doc))\n",
    "print('Word IDs present in dictionary:')\n",
    "print(bow)\n",
    "print([song_dictionary[i[0]] for i in bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stream of bag-of-words vectors\n",
    "class BOWCorpus(object):\n",
    "    def __init__(self, corpus_iter, dictionary):\n",
    "        self.corpus_iter = corpus_iter\n",
    "        self.dictionary = dictionary\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for tokens in self.corpus_iter():\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "\n",
    "song_corpus = BOWCorpus(corpus_iter, song_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 s, sys: 401 ms, total: 29.4 s\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "# store BOW-corpus into a file\n",
    "%time MmCorpus.serialize('./data/song_corpus_bow.mm', song_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(57650 documents, 85449 features, 2982186 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "# load BOW-corpus\n",
    "mm_corpus = MmCorpus('./data/song_corpus_bow.mm')\n",
    "print(mm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fewer documents during training, LDA is slow\n",
    "clipped_corpus = ClippedCorpus(mm_corpus, 10000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.5 s, sys: 662 ms, total: 40.2 s\n",
      "Wall time: 40.7 s\n"
     ]
    }
   ],
   "source": [
    "# fit LDA model\n",
    "%time lda_model = LdaModel(clipped_corpus, num_topics=20, id2word=song_dictionary, passes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"em\" + 0.014*\"mother\" + 0.011*\"roll\" + 0.010*\"brother\" + 0.009*\"doo\" + 0.007*\"father\" + 0.007*\"death\" + 0.007*\"ooo\" + 0.007*\"young\" + 0.007*\"morning\"'),\n",
       " (1,\n",
       "  '0.046*\"wanna\" + 0.041*\"christmas\" + 0.022*\"ah\" + 0.018*\"ha\" + 0.015*\"round\" + 0.012*\"year\" + 0.010*\"merry\" + 0.009*\"burn\" + 0.009*\"drink\" + 0.008*\"shout\"'),\n",
       " (2,\n",
       "  '0.040*\"know\" + 0.029*\"time\" + 0.023*\"ve\" + 0.019*\"like\" + 0.017*\"life\" + 0.015*\"love\" + 0.013*\"got\" + 0.012*\"oh\" + 0.012*\"ll\" + 0.011*\"way\"'),\n",
       " (3,\n",
       "  '0.070*\"want\" + 0.036*\"little\" + 0.014*\"hell\" + 0.011*\"fool\" + 0.011*\"like\" + 0.010*\"knock\" + 0.009*\"bit\" + 0.007*\"break\" + 0.006*\"white\" + 0.006*\"try\"'),\n",
       " (4,\n",
       "  '0.074*\"home\" + 0.065*\"da\" + 0.013*\"ba\" + 0.011*\"going\" + 0.006*\"dee\" + 0.006*\"wah\" + 0.006*\"chickens\" + 0.005*\"houston\" + 0.005*\"dan\" + 0.004*\"thou\"'),\n",
       " (5,\n",
       "  '0.138*\"la\" + 0.065*\"na\" + 0.021*\"pum\" + 0.013*\"bang\" + 0.012*\"pa\" + 0.012*\"sa\" + 0.011*\"ang\" + 0.010*\"di\" + 0.009*\"ng\" + 0.008*\"rum\"'),\n",
       " (6,\n",
       "  '0.077*\"baby\" + 0.071*\"oh\" + 0.046*\"yeah\" + 0.026*\"girl\" + 0.021*\"need\" + 0.020*\"got\" + 0.019*\"like\" + 0.014*\"gonna\" + 0.014*\"know\" + 0.012*\"little\"'),\n",
       " (7,\n",
       "  '0.031*\"man\" + 0.019*\"got\" + 0.016*\"good\" + 0.015*\"better\" + 0.015*\"ve\" + 0.012*\"like\" + 0.010*\"bad\" + 0.009*\"oh\" + 0.008*\"cause\" + 0.008*\"ll\"'),\n",
       " (8,\n",
       "  '0.118*\"ll\" + 0.025*\"world\" + 0.015*\"live\" + 0.015*\"happy\" + 0.012*\"gimme\" + 0.010*\"land\" + 0.009*\"sky\" + 0.008*\"midnight\" + 0.008*\"join\" + 0.007*\"walking\"'),\n",
       " (9,\n",
       "  '0.066*\"let\" + 0.043*\"come\" + 0.019*\"gonna\" + 0.016*\"dance\" + 0.014*\"shine\" + 0.012*\"light\" + 0.012*\"sun\" + 0.012*\"rain\" + 0.010*\"night\" + 0.009*\"praise\"'),\n",
       " (10,\n",
       "  '0.015*\"away\" + 0.014*\"ve\" + 0.013*\"like\" + 0.012*\"way\" + 0.012*\"know\" + 0.011*\"ll\" + 0.011*\"feel\" + 0.010*\"night\" + 0.010*\"heart\" + 0.009*\"eyes\"'),\n",
       " (11,\n",
       "  '0.021*\"big\" + 0.018*\"stop\" + 0.018*\"ready\" + 0.010*\"road\" + 0.009*\"goes\" + 0.008*\"fall\" + 0.007*\"got\" + 0.007*\"easy\" + 0.006*\"talk\" + 0.006*\"higher\"'),\n",
       " (12,\n",
       "  '0.036*\"got\" + 0.023*\"like\" + 0.015*\"ya\" + 0.011*\"money\" + 0.011*\"ain\" + 0.010*\"nigga\" + 0.008*\"know\" + 0.008*\"em\" + 0.008*\"niggas\" + 0.007*\"man\"'),\n",
       " (13,\n",
       "  '0.025*\"rock\" + 0.017*\"music\" + 0.014*\"roll\" + 0.012*\"blues\" + 0.012*\"body\" + 0.011*\"oo\" + 0.010*\"won\" + 0.008*\"dead\" + 0.007*\"songs\" + 0.006*\"rhythm\"'),\n",
       " (14,\n",
       "  '0.047*\"gotta\" + 0.046*\"run\" + 0.017*\"hello\" + 0.013*\"yeah\" + 0.013*\"ma\" + 0.013*\"shame\" + 0.011*\"yea\" + 0.010*\"faster\" + 0.009*\"ho\" + 0.008*\"candy\"'),\n",
       " (15,\n",
       "  '0.019*\"sweet\" + 0.017*\"daddy\" + 0.013*\"black\" + 0.012*\"ride\" + 0.011*\"river\" + 0.008*\"blue\" + 0.007*\"dream\" + 0.006*\"law\" + 0.006*\"old\" + 0.006*\"america\"'),\n",
       " (16,\n",
       "  '0.021*\"dog\" + 0.009*\"je\" + 0.007*\"est\" + 0.006*\"forgive\" + 0.006*\"arise\" + 0.006*\"ne\" + 0.006*\"lion\" + 0.006*\"cheek\" + 0.005*\"matilda\" + 0.005*\"les\"'),\n",
       " (17,\n",
       "  '0.043*\"lord\" + 0.039*\"god\" + 0.030*\"jesus\" + 0.019*\"glory\" + 0.016*\"holy\" + 0.015*\"uh\" + 0.011*\"king\" + 0.010*\"sing\" + 0.008*\"born\" + 0.008*\"hear\"'),\n",
       " (18,\n",
       "  '0.051*\"hey\" + 0.022*\"fuck\" + 0.016*\"ain\" + 0.016*\"bitch\" + 0.013*\"shit\" + 0.013*\"cause\" + 0.013*\"like\" + 0.010*\"woman\" + 0.008*\"gonna\" + 0.007*\"wanna\"'),\n",
       " (19,\n",
       "  '0.171*\"love\" + 0.025*\"heart\" + 0.018*\"ll\" + 0.015*\"ooh\" + 0.014*\"know\" + 0.013*\"hold\" + 0.012*\"let\" + 0.011*\"walk\" + 0.010*\"right\" + 0.010*\"need\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lord', 0.0431258),\n",
       " ('god', 0.038804255),\n",
       " ('jesus', 0.029804338),\n",
       " ('glory', 0.018821897),\n",
       " ('holy', 0.015902156),\n",
       " ('uh', 0.01526181),\n",
       " ('king', 0.0107914265),\n",
       " ('sing', 0.009564806),\n",
       " ('born', 0.008407606),\n",
       " ('hear', 0.008236479),\n",
       " ('hallelujah', 0.008143266),\n",
       " ('freedom', 0.008082764),\n",
       " ('mighty', 0.007882832),\n",
       " ('earth', 0.007433106),\n",
       " ('worship', 0.0073015536),\n",
       " ('heaven', 0.0068215164),\n",
       " ('shall', 0.0068201623),\n",
       " ('war', 0.006256063),\n",
       " ('huh', 0.0061148163),\n",
       " ('peace', 0.0059593124),\n",
       " ('soul', 0.0056202076),\n",
       " ('christ', 0.0055074594),\n",
       " ('angels', 0.0050303647),\n",
       " ('free', 0.0048797955),\n",
       " ('men', 0.004693255),\n",
       " ('thy', 0.0045556896),\n",
       " ('chorus', 0.0044774497),\n",
       " ('stand', 0.0044649006),\n",
       " ('spirit', 0.00418655),\n",
       " ('thee', 0.0038032983),\n",
       " ('pray', 0.003653935),\n",
       " ('power', 0.0035389818),\n",
       " ('children', 0.0034555),\n",
       " ('bum', 0.003356569),\n",
       " ('nah', 0.00334771),\n",
       " ('great', 0.003335568),\n",
       " ('calling', 0.0032657867),\n",
       " ('battle', 0.0032543887),\n",
       " ('mercy', 0.0032310637),\n",
       " ('son', 0.003163857),\n",
       " ('cross', 0.0031271162),\n",
       " ('united', 0.0030439259),\n",
       " ('grave', 0.0030358825),\n",
       " ('hands', 0.0030163608),\n",
       " ('bridge', 0.0029888086),\n",
       " ('er', 0.002908207),\n",
       " ('oh', 0.0028648286),\n",
       " ('diggy', 0.0027998555),\n",
       " ('saints', 0.0027133592),\n",
       " ('paris', 0.0026958317)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topic(17, topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
